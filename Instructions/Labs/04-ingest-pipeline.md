---
lab:
  title: 在 Microsoft Fabric 中使用管道引入数据
  module: Use Data Factory pipelines in Microsoft Fabric
---

# 在 Microsoft Fabric 中使用管道引入数据

数据湖屋是用于云规模分析解决方案的常见分析数据存储。 数据工程师的核心任务之一是实现将数据从多个操作数据源引入湖屋并进行管理。 在 Microsoft Fabric 中，可以通过创建管道实现“提取、转换和加载”(ETL) 或“提取、加载和转换”(ELT) 解决方案，以引入数据  。

Fabric 还支持 Apache Spark，使你能够编写和运行代码以大规模处理数据。 通过组合 Fabric 中的管道和 Spark 功能，可以实现复杂的数据引入逻辑，将数据从外部源复制到湖屋所基于的 OneLake 存储中，然后先使用 Spark 代码执行自定义数据转换，再将其加载到表中进行分析。

完成本实验室大约需要 60 分钟。

> **注意**：完成本练习需要 Microsoft Fabric 试用版。[](https://learn.microsoft.com/fabric/get-started/fabric-trial)

## 创建工作区

在 Fabric 中处理数据之前，创建一个已启用的 Fabric 试用版的工作区。

1. 在 [Microsoft Fabric 主页](https://app.fabric.microsoft.com)中，选择“Synapse 数据工程”。****
1. 在左侧菜单栏中，选择“工作区”（图标类似于 &#128455;）。
1. 新建一个工作区并为其指定名称，并选择包含 Fabric 容量（试用版、高级版或 Fabric）的许可模式  。
1. 打开新工作区时，它应为空。

    ![Fabric 中空工作区的屏幕截图。](./Images/new-workspace.png)

## 创建湖屋

现在已经有了工作区，可以创建数据湖屋，将数据引入其中了。

1. 在“Synapse 数据工程”主页中，新建湖屋并为其指定名称 。

    大约一分钟后，将完成创建一个不包含表或文件的新湖屋 。

1. 在左侧窗格的“湖视图”选项卡上，在“Files”节点的“...”菜单中，选择“新建子文件夹”并创建名为“new_data”的子文件夹    。

## 创建管道

引入数据的一种简单方法是使用管道中的“复制数据”活动从源中提取数据并将其复制到湖屋中的文件。

1. 在湖屋的“主页”上，选择“获取数据”，再选择“新建数据管道”，并创建名为“引入销售数据”的新数据管道****************。
2. 如果“**复制数据**”向导未自动打开，请在管道编辑器页中选择“**复制数据>使用复制助手**”。
3. 在“**复制数据**”向导的“**选择数据源**”页上，在搜索栏中键入 HTTP，然后在“**新源**”部分选择“**HTTP**”。


    ![“选择数据源”页的屏幕截图。](./Images/choose-data-source.png)

4. 在“**连接到数据源**”窗格中，输入以下数据源连接设置：
    - **URL**：`https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv`
    - 连接：创建新连接
    - 连接名称：指定一个唯一名称
    - 数据网关：（无）****
    - 身份验证类型：匿名
5. 选择**下一步**。 然后确保已选择以下设置：
    - 相对 URL：留空
    - 请求方法：GET
    - 其他标题：留空
    - 二进制副本：处于<u>未</u>选择状态
    - 请求超时：留空
    - 最大并发连接数：留空
6. 选择“下一步”，等待数据采样，并确保选择以下设置：
    - **文件格式**： DelimitedText
    - 列分隔符：逗号 (,)
    - 行分隔符：换行符 (\n)
    - 首行用作标题：已选择
    - 压缩类型：无
7. 选择“预览数据”，查看要引入的数据的示例。 然后关闭数据预览并选择“下一步”。
8. 在“**选择数据目标**”页上，选择“**OneLake 数据中心**”，然后选择“现有湖屋”。
9. 设置以下数据目标选项，然后选择“下一步”：
    - 根文件夹：Files
    - 文件夹路径名称：new_data
    - 文件名：sales.csv
    - 复制行为：无
10. 设置以下文件格式选项，然后选择“下一步”：
    - **文件格式**： DelimitedText
    - 列分隔符：逗号 (,)
    - 行分隔符：换行符 (\n)
    - 将标题添加到文件：已选择
    - 压缩类型：无
11. 在“复制摘要”页上，查看复制操作的详细信息，然后选择“保存 + 运行” 。

    将创建一个包含“复制数据”活动的新管道，如下所示：

    ![包含“复制数据”活动的管道的屏幕截图。](./Images/copy-data-pipeline.png)

12. 管道开始运行时，可以在管道设计器下的“输出”窗格中监视其状态。 使用 &#8635;（刷新）图标刷新状态，并等待它成功。

13. 在左侧菜单栏中，选择你的湖屋。
14. 在主页的“湖屋资源管理器”窗格中，展开“Files”并选择“new_data”文件夹，以验证是否已复制 sales.csv 文件    。

## 创建笔记本

1. 在湖屋的主页上，在“打开笔记本”菜单中，选择“新建笔记本”  。

    几秒钟后，一个包含单个单元格的新笔记本将会打开。 笔记本由一个或多个单元格组成，这些单元格可以包含代码或 markdown（格式化文本） 。

2. 在笔记本中选择包含一些简单代码的现有单元格，然后将默认代码替换为以下变量声明。

    ```python
   table_name = "sales"
    ```

3. 在单元格的“...”菜单（位于单元格右上角）中，选择“切换参数单元格” 。 这会配置单元格，以便在从管道运行笔记本时将其中声明的变量视为参数。

4. 在参数单元格下，使用“+ 代码”按钮添加新的代码单元格。 然后，向其中添加以下代码：

    ```python
   from pyspark.sql.functions import *

   # Read the new sales data
   df = spark.read.format("csv").option("header","true").load("Files/new_data/*.csv")

   ## Add month and year columns
   df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

   # Derive FirstName and LastName columns
   df = df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

   # Filter and reorder columns
   df = df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "EmailAddress", "Item", "Quantity", "UnitPrice", "TaxAmount"]

   # Load the data into a table
   df.write.format("delta").mode("append").saveAsTable(table_name)
    ```

    此代码加载由“复制数据”活动引入的 sales.csv 文件中的数据，应用一些转换逻辑，并将转换后的数据保存为表 - 如果表已存在，则追加数据。

5. 验证你的笔记本是否与此类似，然后使用工具栏上的“&#9655; 全部运行”按钮运行其包含的所有单元格。

    ![笔记本的屏幕截图，其中包含参数单元格和用于转换数据的代码。](./Images/notebook.png)

    > 注意：由于这是你第一次在此会话中运行 Spark 代码，因此必须启动 Spark 池。 这意味着第一个单元格可能需要一分钟左右才能完成。

6. 笔记本运行完成后，在左侧的“湖屋资源管理器”窗格中，在“Tables”的“...”菜单中选择“刷新”，并验证是否已创建 sales 表    。
7. 在笔记本菜单栏中，使用 ⚙️“设置”图标查看笔记本设置。 然后将笔记本的“名称”设置为“Load Sales”并关闭设置窗格 。
8. 在左侧的中心菜单栏中，选择你的湖屋。
9. 在“资源管理器”窗格中，刷新视图。 然后展开“Tables”，选择“sales”表以预览其包含的数据 。

## 修改管道

现在已实现了一个笔记本来转换数据并将其加载到表中，接下来可以将笔记本合并到管道中以创建可重用的 ETL 流程。

1. 在左侧的中心菜单栏中，选择之前创建的“引入销售数据”管道。
2. 在“活动”选项卡上的“更多活动”列表中，选择“删除数据”  。 然后将新的“删除数据”活动置于“复制数据”活动的左侧，并将其“完成时”输出连接到“复制数据”活动，如下所示   ：

    ![包含“删除数据”和“复制数据”活动的管道的屏幕截图。](./Images/delete-data-activity.png)

3. 选择“删除数据”活动，然后在设计画布下方的窗格中设置以下属性：
    - 常规：
        - 名称：删除旧文件
    - **Source**
        - 连接****：你的湖屋**
        - 文件路径类型：通配符文件路径
        - 文件夹路径：Files/new_data
        - 通配符文件名：*.csv        
        - 递归：已选中
    - 日志记录设置：
        - 启用日志记录：未选中 *<u></u>*

    这些设置可确保在复制 sales.csv 文件之前删除所有现有的 .csv 文件。

4. 在管道设计器的“活动”选项卡上，选择“笔记本”，将“笔记本”活动添加到管道  。
5. 选择“复制数据”活动，然后将其“完成时”输出连接到“笔记本”活动，如下所示  ：

    ![包含“复制数据”和“笔记本”活动的管道的屏幕截图。](./Images/pipeline.png)

6. 选择“笔记本”活动，然后在设计画布下方的窗格中设置以下属性：
    - 常规：
        - 名称：加载 Sales 笔记本
    - 设置：
        - 笔记本：加载 Sales
        - 基本参数：添加具有以下属性的新参数：
            
            | 名称 | 类型 | Value |
            | -- | -- | -- |
            | table_name | 字符串 | new_sales |

    table_name 参数将传递给笔记本，并替代分配给参数单元格中 table_name 变量的默认值 。

7. 在“开始”选项卡上，使用 &#128427;（保存）图标保存管道 。 然后使用“&#9655; 运行”按钮运行管道，并等待所有活动完成。

    ![包含“数据流”活动的管道的屏幕截图。](./Images/pipeline-run.png)

> 注意：如果收到错误消息 *只能在湖屋的上下文中执行 Spark SQL 查询。请附加湖屋以继续操作*：请打开笔记本，在左侧窗格中造择已创建的湖屋，选择“移除所有湖屋”，然后再次添加它。**** 返回到管道设计器并选择“&#9655; 运行”****。

8. 在门户左边缘的中心菜单栏中选择湖屋。
9. 在“资源管理器”窗格中，展开“Tables”，选择“new_sales”表以预览其包含的数据  。 此表是在管道运行笔记本时由笔记本创建的。

在本练习中，你实现了一个数据引入解决方案，该解决方案使用管道将数据从外部源复制到湖屋，然后使用 Spark 笔记本转换数据并将其加载到表中。

## 清理资源

在本练习中，你学习了如何在 Microsoft Fabric 中实现管道。

如果已完成湖屋探索，可删除为本练习创建的工作区。

1. 在左侧栏中，选择工作区的图标以查看其包含的所有项。
2. 在工具栏上的“...”菜单中，选择“工作区设置” 。
3. 在“常规”部分中，选择“删除此工作区”。********
